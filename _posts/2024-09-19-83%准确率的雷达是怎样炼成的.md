---
title: 83%准确率的雷达是怎样炼成的
data: 2024-09-19 20:54:02 +0800
categories: [RoboMaster, 雷达]
tags: [RoboMaster, CUDA, TensorRT, YOLO, 计算机视觉, 雷达, 点云聚类, 目标跟踪, PCL, DBSCAN, 卡尔曼滤波, 辛格模型, Auction 算法, SORT, DeepSORT, VTune Profiler, Nsight Compute]
description: 雷达站识别中如何对识别、定位和跟踪进行优化
math: true
author: zmsbruce
---

![](/assets/img/2024-09-19-83%25准确率的雷达是怎样炼成的/cover.png)

# 雷达站都做了哪些优化

雷达相较于去年的优化分为识别、定位、跟踪三个方面。在识别方面，引入了 CUDA 来加速神经网络模型的前处理和后处理、利用了 TensorRT 部署 YOLO 网络、并通过多流处理提高处理效率，使得可以使用参数更大的模型，如 YOLOv8m. 

在定位方面，通过使用聚类算法来解决噪点干扰问题。原始方法由于激光雷达的非重复扫描特性和噪点的干扰，导致定位不准确。为了改善这一点，使用了 DBSCAN 算法进行点云聚类，并识别出噪声并将其分类为噪声点。

在跟踪方面，将跟踪步骤移至定位之后，并弃用了之前的 DeepSORT 算法，采用了针对三维坐标的新跟踪算法。此外，引入了辛格模型处理目标加速度的变化，以及采用 Auction 算法进行数据关联，优化了匹配过程。整体算法流程类似于 SORT，包括检测、预测、数据关联、更新以及轨迹的创建与删除，以实现更高效和准确的目标跟踪。

# 识别优化

## 为什么要优化

从结果而言，去年的雷达效果不太尽人意，我们推测这其中一部分原因可能是神经网络的识别问题。而想要改善这一点，除了其架构上的改进（比如从 yolov5 升级到 v8，乃至新出的 v9），参数规模上的增加（从 n 升级到 s 或 m，甚至更大）也是很大的一方面。然而，由于每一帧的处理时间太长，导致其掣肘了更大规模神经网络的使用，因此对处理进行加速是必要的。

去年的雷达工程使用 TensorRT 对 YOLO 网络进行部署，比较充分利用了 GPU 资源。然而对图像的预处理以及对检测结果的后处理却仅仅使用 CPU，颇有一种吕布骑狗的美感，因为输入图像的大小（以第一层推理为例）是 2592\*2048\*3，检测结果的大小也有 84\*8400，而使用 CPU 进行图像的处理，纵使是使用向量寄存器和多线程，速度还是不太能看。而 CUDA 所拥有的 SIMT 的特性能够使其可以同时执行成千上万的线程，大幅度提升处理速度，从而节省时间，是替代之前前后处理的理想方案。

此外，在 GPU 进行 CUDA 预处理、后处理和推理的时候，CPU 端是处于空闲状态的，这允许 CPU 进行其它一些操作（如点云处理和聚类），流程如下图所示：

![](/assets/img/2024-09-19-83%25准确率的雷达是怎样炼成的/detect_flow.png)
_雷达站识别流程_

总而言之，使用 CUDA 加速非常有必要，**CUDA，启动！**

## 优化的内容

### 前处理

为了满足输入到 YOLO 网络的需求，其需要的前处理包括：图像缩放 → 图像填充 → 归一化 → 通道转换（BGR → RGB）及平面化（RGBRGBRGB → RRRGGGBBB）。这些都是点对点的操作，比较容易通过 CUDA 进行功能的实现。

### 后处理

YOLOv8 网络的输出维度为 84\*8400，其中 84 由 bbox(x, y, width, height) 和 80 个类别组成。8400 为潜在的检测条目数。其后处理包括解码（从 80 个类别中选择其置信度最高的一个，并标记其置信度），和非极大值抑制（NMS），并过滤出最终的结果。其中，NMS 是比较难进行 CUDA 优化的，因为其涉及了较多的分支语句，而这并不是 CUDA 所擅长的。

### Batch

对于装甲板的推理，我们使用了动态输入的 TensorRT 引擎来进行加速。因此我们需要处理多张图片并加载到网络的输入端，而不是一个一个地进行推理。如果我们能够同时处理一个 batch 的多张图片，便会产生显著的性能提升。对于具体的实现，我们使用了 CUDA Stream 让不同流中的命令并发执行，以达到性能优化的目的。

## 优化的性能

### 时间

下面的表格记录了完整推理一张图片（包括车和装甲板）的时间对比，在这里使用了 v8s 的 TensorRT 引擎进行推理，显卡为 NVIDIA GeForce RTX 3060 Ti，可以看出加速的效果是显而易见的：

| 方法                         | 耗时  |
| ---------------------------- | :---: |
| 不进行加速                   | 38ms  |
| 使用 CUDA 加速预处理         | 18ms  |
| 使用 CUDA 加速预处理和后处理 |  9ms  |

我们尝试了使用更大的模型 v8m 对 v8s 进行替换，得到其耗时如下:

| 模型  | 耗时  |
| :---: | :---: |
|  v8s  |  9ms  |
|  v8m  | 20ms  |

### 整体分析

我们使用 Intel VTune Profiler 对函数执行时间进行统计与分析，分析代码为去掉 `imshow` 函数的 [sample](https://github.com/zmsbruce/rm_radar/blob/main/samples/main.cpp) 得到的结果如下所示：

![](/assets/img/2024-09-19-83%25准确率的雷达是怎样炼成的/profile_total.png)
_整体耗时分析_

可以看出，处理一张的耗时和处理一个 batch 的耗时相差不大，这要归功于我们我们使用 CUDA 多流进行的优化，虽然车的图片大小远小于场地的图片也可能是造成这个结果的因素。

为了保险起见，我们修改代码，使得一个 batch 的前后处理函数只使用一个流，并利用原代码和修改后的代码统计处理单张图片与处理一个 batch 的图片的时间比，得到的结果如下表所示，说明了多流优化的确起到了作用：

|   方法   |  时间比  |
| :------: | :------: |
| 使用多流 | 0.70 : 1 |
| 使用多流 | 0.52 : 1 |

此外，对于一个 `Detector::detect` 函数，其各个函数的执行时间如下：

![](/assets/img/2024-09-19-83%25准确率的雷达是怎样炼成的/profile_detect.png)
_detect 函数耗时分析_

可以看出预处理、后处理和推理的函数运行时间相当，这也是我们所期望的、令人欣慰的结果。

### 核函数分析

我们使用 NVIDIA Nsight Compute 对 CUDA 核函数进行测试与分析。使用 Nsight Compute 打开此文件就可以查看有关核函数的报告，包括 GPU 性能指标、CUDA指令分析、内存访问分析、Warp分析等，其包括分析报告和可视化，非常详细。然而由于**本人对 CUDA 性能优化认知尚浅**，只能在此给出粗略的观察、推测与优化建议，希望之后有对这方面了解更深的人着手解决这个问题：

#### 性能瓶颈

`resizeKernel`, `copyMakeBorderKernel` 性能受限于内存访问速度，而 `transposeKernel` 和 `decodeKernel` 受限于延迟问题。`blobKernel` 和 `NMSKernel` 达到了良好的设备性能。其具体指标如下所示：

![](/assets/img/2024-09-19-83%25准确率的雷达是怎样炼成的/profile_kernel.png)

一些公共的问题如下所示：

* **Uncoalesced Global Accesses**：Memory Coalescing 利用了这样一个事实，在任何给定的时间点上，warp中的线程都执行相同的指令，那么当warp中的所有线程访问全局内存的位置是连续的时，将实现最佳的内存访问模式。

|        核函数        | 加速比 |
| :------------------: | :----: |
|     resizeKernel     | 89.15% |
| copyMakeBorderKernel | 63.21% |
|      blobKernel      | 26.74% |
|   transposeKernel    | 77.37% |
|     decodeKernel     | 79.39% |
|      NMSKernel       | 50.07% |

* **Long Scoreboard Stalls**：这是由于读取 offchip 上(如 Local, Global 上的 memory )上的数据并使用，由于这些数据访问代价高，要长时间等待。

|        核函数        | 加速比 |
| :------------------: | :----: |
|     resizeKernel     | 36.16% |
| copyMakeBorderKernel | 42.67% |
|      blobKernel      | 22.80% |
|   transposeKernel    |  nan   |
|     decodeKernel     | 43.97% |
|      NMSKernel       | 27.56% |

#### 之后的优化建议

- **核函数可能过于简单**：可能会导致一直在读取或写入设备内存。将前处理中的几个核函数，以及后处理中的 `transposeKernel` 和 `decodeKernel` 合并可能会缓解这个问题；
- **减少访存次数**：使用 `float4`, `int4` 等访存方式可以做到合并访问，加速处理，例如对像素可以在一个 thread 中使用 `uchar4*` 访问；
- **对 resizeKernel 使用纹理内存**：纹理内存具有硬件缓存和硬件双线性插值的特性，能够加快图像缩放的速度；

> 纹理内存仅支持 RGBA 格式，输入图像为 BGR 格式，需要编写 CUDA kernel 或者在 CPU 端转换格式，可能耗费的时间会多于使用纹理内存节省的时间。
{: .prompt-warning}

## 参考

* [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)
* [图文并茂，超详细解读nms cuda拓展源码](https://zhuanlan.zhihu.com/p/466169614)
* [Nsight Kernel Profiling Guide](https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html)
* [CUDA 编程之 Memory Coalescing](https://zhuanlan.zhihu.com/p/300785893)
* [使用Nsight Compute 找到常见Stall原因](https://zhuanlan.zhihu.com/p/464172074)

# 定位优化

## 为什么要聚类

未优化前的定位机器人使用的算法如下图所示，其首先将输入的点云转换为深度图，然后积累背景图，再将当前深度图和背景图进行相减得到前景图，最后根据先前检测结果提取前景图中的点，转化成三维坐标。对于最后一步，其将深度图中矩形框框住的所有点按深度进行排序，将头尾抛弃一部分后取所有点的均值。

![](/assets/img/2024-09-19-83%25准确率的雷达是怎样炼成的/old_locating.png)
_23年雷达定位算法_

然而，上述方法的定位会受到噪点的干扰而导致其不准确，尤其是在矩形框内包含场地边缘的噪点时。具体而言，采用的激光雷达 Livox Horizon 采用非重复扫描，其 FOV 与积分时间有关，如下图所示：

![](/assets/img/2024-09-19-83%25准确率的雷达是怎样炼成的/lidar_fov.png)
_Livox雷达FOV与积分时间的关系_

在当前采用的积分时间 100ms的情况下（算法使用一个队列保存最近的三张深度图，相机帧率为 30Hz），其 FOV 覆盖率不到 60%，因此会造成激光雷达扫描到车身上的点云时多时少。然而噪点却始终保持一定程度上的数量（为什么？），这就造成了使用去年的方法在矩形框包含场地边缘时，定位会受到噪点的干扰而导致其不准确。在[雷达站23赛季总结和24赛季技术展望](https://www.notion.so/hitcrt2024vision/23-24-9e7427630f8b463aa3e2d7cb09fe9b2f)中也有相应的叙述。某一次实验结果如下图所示，x 坐标为帧数，y 坐标为车身点云数量：

![](/assets/img/2024-09-19-83%25准确率的雷达是怎样炼成的/noise_point_num.png)
_车身点云数量变化_

由于噪点相对于车身点的数量较低，因此**如果使用聚类算法，每次取点数最多的一类，可以在某种程度上缓解定位不准确的问题**（但不能完全解决，原因在后面会解释）。

## 聚类 v1.0

第一版方法针对聚类算法使用了 `pcl::EuclideanClusterExtraction` ，这个方法使用现成的 api，比较容易实现。然而由于当时思路的局限性，其实现的具体方法比较繁琐，如下图所示：

![](/assets/img/2024-09-19-83%25准确率的雷达是怎样炼成的/cluster_v1.png)
_聚类 1.0 算法_

## 聚类 v2.0

与 v1.0 版本对比，我们进行了如下两处改进：

* 使用 DBSCAN 代替 `pcl::EuclideanClusterExtraction`：DBSCAN 能够将噪声识别出来并分类为噪声点，而 `pcl::EuclideanClusterExtraction` 通常需要先对噪声进行过滤。
* 直接对深度图的像素点进行聚类：我们发现坐标系的转换是不必要的，将其删除可以减少内存拷贝，提高处理速度；

结果如下图所示，可以其聚类和噪声识别的效果均达到了较好的水平：

![](/assets/img/2024-09-19-83%25准确率的雷达是怎样炼成的/cluster_v2_result.png)

![](/assets/img/2024-09-19-83%25准确率的雷达是怎样炼成的/origin_image.png)
_聚类 2.0 效果_

## 问题

仅仅依靠聚类无法完全解决噪点问题。由之前的[实验数据](/assets/img/2024-09-19-83%25准确率的雷达是怎样炼成的/noise_point_num.png)可知，一个车身的点的数量波动非常大，最低可能小于20个点，这就导致了矩形框框住的车身点数量有时会小于噪点数量，从而导致这种情况下定位不准确。然而如果直接忽略掉噪声是一种非常冒险的行为，因为这可能导致被误判为噪点的正常点也被忽略。

一种解决办法是制定**跟踪策略**，与聚类方法一起解决噪点带来的问题。

# 跟踪优化

## 为什么要优化

去年的跟踪方法是放在检测之后定位之前，使用了 DeepSORT 进行检测框的跟踪。从实际效果上看，这是一个比较鸡肋的做法：

* 定位中的二维检测框实际上并不需要特别精确，有时为了搜寻更多的点反而需要扩大搜索范围，即放大搜索框，这导致了对二维检测框进行卡尔曼滤波并没有多大的意义；
* 然而目标跟踪能够对误识别进行有效的抑制，即当某一帧或几帧对一个机器人的识别出现了错误时，DeepSORT 能够根据检测框和历史特征进行正确的匹配，并进一步修改其为正确的类别；

虽然使用 DeepSORT 进行目标跟踪并没有完全利用其功能，但仅仅使用其当作类别的稳定器也未尝不可。真正导致需要优化的原因是**定位的跳变**，即在检测框包含场地边缘时，定位会受到噪点的干扰而导致其不准确。

为了解决上述问题，同时使得跟踪算法得到更完全的利用，我们将跟踪转移到定位之后，并不再套用 DeepSORT 算法，而是利用其思想，构造出针对定位后的三维坐标的跟踪算法。

## 滤波器的选择

辛格模型（Singer model）是一种用于描述目标在雷达或追踪系统中的加速度变化的数学模型。这种模型特别适用于处理目标的机动性，例如在空中或海上的飞机、导弹和船只等，它们的加速度不是恒定的，而是随时间变化的。辛格模型通过引入一个随机过程来模拟这种加速度变化，从而提高了跟踪滤波器在面对机动目标时的性能。

### 辛格模型的基本形式

在辛格模型中，目标的加速度被建模为一阶马尔科夫过程，即具有指数相关的随机过程。该模型的数学表示如下：

设 $$a(t)$$ 为时间 $$t$$ 时的加速度，辛格模型假设 $$a(t)$$ 遵循以下随机微分方程：

$$
\frac{da(t)}{dt} = -\frac{1}{\tau} a(t) + \sigma \sqrt{\frac{2}{\tau}} w(t)
$$

其中：
- $$\tau$$ 是相关时间，表示加速度保持相关的时间长度。
- $$\sigma$$ 是加速度过程的标准差，表示加速度的变化程度。
- $$w(t)$$ 是标准白噪声过程。

### 模型解释

在辛格模型中，第一项 $$-\frac{1}{\tau} a(t)$$ 表示加速度会自然地衰减到零，而这种衰减是以 $$\tau$$ 为时间常数的指数衰减过程。第二项 $$\sigma \sqrt{\frac{2}{\tau}} w(t)$$ 表示新的随机加速度被加入到系统中，其中 $$\sigma$$ 控制加速度变化的振幅，$$\sqrt{\frac{2}{\tau}}$$ 确保整个过程的功率谱密度保持恒定。

### 优点

* 模型适合：辛格模型用于描述目标在雷达或追踪系统中的加速度变化，正好符合我们的需求；
* 参数少：只需要调整 $$\sigma$$ 和 $$\tau$$ 这两个参数；

## 匹配算法

Auction 算法是一种用于解决分配问题的有效算法，特别是在求解带权重的二分匹配问题中表现突出。这种算法以其竞拍（或拍卖）的过程命名，其中买家（或对象）和卖家（或任务）通过一系列迭代竞标来达到最优匹配。Auction 算法由 Dimitri Bertsekas 在 1980 年代提出，它是一种快速的离散优化方法，尤其适用于大规模问题。

### 基本概念

在典型的分配问题中，我们有一组买家和一组卖家，每个买家对每个卖家有一个特定的价值或偏好。目标是找到买家和卖家之间的最优匹配，使得总体价值最大化或总成本最小化。

### 算法步骤

Auction 算法的核心思想是模拟一个拍卖过程，其中买家对卖家进行出价。算法的基本步骤包括：

1. **初始化**：
   - 每个买家开始时都没有匹配的卖家。
   - 每个卖家设置一个价格，初始价格可以是0或其他基线值。

2. **竞拍阶段**：
   - 每个未匹配的买家考虑所有卖家，并基于当前的价格和自己对卖家的评估（价值减去价格）选择最合算的卖家进行出价。
   - 买家会出一个比当前价格稍高的价格，这个增幅称为“出价增量”。

3. **分配阶段**：
   - 卖家会收到来自一个或多个买家的出价。如果有多个出价，卖家会选择出价最高的买家，并更新自己的价格为这个出价。
   - 被选中的买家与该卖家匹配，之前与该卖家匹配的买家（如果有的话）变为未匹配状态。

4. **终止条件**：
   - 当所有买家都成功匹配后，算法结束。
   - 如果有买家未匹配，返回到竞拍阶段。

### 特点和优势

- **效率**：Auction算法特别适合大规模问题，因为它允许并行处理和快速迭代。
- **灵活性**：可以调整出价策略和增量，优化算法的性能。
- **实现简单**：算法结构清晰，易于实现。

## 整体算法

整体算法与 SORT 近似：

1. **检测**：在每一帧中，首先检测出每个机器人的三维位置坐标和类别；

2. **预测**：对于已经在跟踪中的每个目标，使用卡尔曼滤波器预测其在当前帧中的位置；

3. **数据关联**：使用当前帧中的检测结果和预测的跟踪位置来进行数据关联。关联评分通过计算位置和类别按照一定比例加权得到。

4. **更新**：根据匹配结果，更新每个跟踪目标的状态。如果一个跟踪目标在当前帧有匹配的检测结果，则卡尔曼滤波器使用这个新的检测数据来校正目标的位置和类别。

5. **创建和删除轨迹**：对于那些在当前帧中被检测到但没有与任何现有跟踪匹配的检测框，初始化新的跟踪器。如果一个跟踪目标在多个连续帧中没有匹配的检测结果，认为该目标已经丢失，因此将其从跟踪列表中删除。

# 代码

哈尔滨工业大学 2024 年雷达站核心库代码：[https://github.com/zmsbruce/rm_radar](https://github.com/zmsbruce/rm_radar)
